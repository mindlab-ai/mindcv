{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "[MindCV](https://github.com/mindspore-lab/mindcv) is an open source toolbox for computer vision research and development based on [MindSpore](https://www.mindspore.cn/en). It collects a series of classic and SoTA vision models, such as ResNet and SwinTransformer, along with their pretrained weights. SoTA methods such as AutoAugment are also provided for performance improvement. With the decoupled module design, it is easy to apply or adapt MindCV to your own CV tasks. In this tutorial, we will provide a quick start guideline for MindCV.\n",
    "\n",
    "This tutorial will take DenseNet classification model as an example to implement migration training for Cifar10 dataset, and explain the usage of MindCV modules in this process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing MindCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://100.125.0.87:32021/repository/pypi/simple\n",
      "Collecting git+https://github.com/mindspore-lab/mindcv.git\n",
      "  Cloning https://github.com/mindspore-lab/mindcv.git to /tmp/pip-req-build-qnvkj8tb\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/mindspore-lab/mindcv.git /tmp/pip-req-build-qnvkj8tb\n",
      "  Resolved https://github.com/mindspore-lab/mindcv.git to commit 858fb89d5ee219be9e9ded86aaa15df06e9c9df5\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from mindcv==0.0.2a0) (1.21.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from mindcv==0.0.2a0) (5.3.1)\n",
      "Requirement already satisfied: tqdm in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from mindcv==0.0.2a0) (4.46.1)\n",
      "Building wheels for collected packages: mindcv\n",
      "  Building wheel for mindcv (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mindcv: filename=mindcv-0.0.2a0-py3-none-any.whl size=165032 sha256=4e8c1f44ded45364658c6aa78f5e25025ba0cae023b33b402c6bdf4266983aa7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-q0tczanu/wheels/a8/17/96/9462c098d9c01564ef506e6666cb48246599c644a849c6aa62\n",
      "Successfully built mindcv\n",
      "Installing collected packages: mindcv\n",
      "Successfully installed mindcv-0.0.2a0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install MindCV from git repo\n",
    "!pip install git+https://github.com/mindspore-lab/mindcv.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The following tutorials assume that all dependent packages have been installed. If you encounter dependency problems, please follow the [installation guide](https://github.com/mindspore-lab/mindcv#dependency) on Git repo. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the [create_dataset](https://mindcv.readthedocs.io/en/latest/api/mindcv.data.html#mindcv.data.create_dataset)  module in [mindcv.data](https://mindcv.readthedocs.io/en/latest/api/mindcv.data.html), we can quickly load standard datasets or customized datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170052608B [01:13, 2328662.39B/s]                                \n"
     ]
    }
   ],
   "source": [
    "from mindcv.data import create_dataset, create_transforms, create_loader\n",
    "import os\n",
    "\n",
    "# dataset path\n",
    "cifar10_dir = './datasets/cifar/cifar-10-batches-bin' # your dataset path\n",
    "num_classes = 10 # num of classes\n",
    "num_workers = 8 # Number of parallel workers\n",
    "download = not os.path.exists(cifar10_dir)\n",
    "\n",
    "# create dataset\n",
    "dataset_train = create_dataset(name='cifar10', root=cifar10_dir, split='train', shuffle=True, num_parallel_workers=num_workers, download=download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[create_dataset](https://mindcv.readthedocs.io/en/latest/api/mindcv.data.html#mindcv.data.create_dataset) parameters:\n",
    "\n",
    "- name: dataset name like mnist, cifar10, imagenet, ' ' means a customized dataset. Default: ' '.\n",
    "\n",
    "- dataset_dir: dataset root dir. Default: './'.\n",
    "\n",
    "- split: data split, ' ' or split name string (train/val/test), if it is ' ', no split is used. Otherwise, it is a subfolder of root dir, e.g., train, val, test. Default: ‘train’.\n",
    "\n",
    "- shuffle: whether to shuffle the dataset. Default: True.\n",
    "\n",
    "- num_sample: Number of elements to sample (default=None, which means sample all elements).\n",
    "\n",
    "- num_shards: Number of shards that the dataset will be divided into (default=None). When this argument is specified, num_samples reflects the maximum sample number of per shard.\n",
    "\n",
    "- shard_id: The shard ID within num_shards (default=None). This argument can only be specified when num_shards is also specified.\n",
    "\n",
    "- num_parallel_workers: Number of workers to read the data (default=None, set in the config).\n",
    "\n",
    "- download: whether to download the dataset. Default: False.\n",
    "\n",
    "- num_aug_repeats: Number of dataset repeatition for repeated augmentation. If 0 or 1, repeated augmentation is diabled. Otherwise, repeated augmentation is enabled and the common choice is 3. (Default: 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing and Loading\n",
    "\n",
    "1. Through the [create_transforms](https://mindcv.readthedocs.io/en/latest/api/mindcv.data.html#mindcv.data.create_transforms) function, you can directly obtain the appropriate data processing augmentation strategies (transform list) for standard datasets, including common data processing strategies on Cifar10 and Imagenet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create transforms\n",
    "trans = create_transforms(dataset_name='cifar10', image_resize=224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[create_transforms](https://mindcv.readthedocs.io/en/latest/api/mindcv.data.html#mindcv.data.create_transforms) parameters:\n",
    "\n",
    "- dataset_name: if ‘ ’, customized dataset. Currently, apply the same transform pipeline as ImageNet. if standard dataset name is given including imagenet, cifar10, mnist, preset transforms will be returned. Default: ‘ ’.\n",
    "\n",
    "- image_resize: the image size after resize for adapting to network. Default: 224.\n",
    "\n",
    "- is_training:  if True, augmentation will be applied if support. Default: False.\n",
    "\n",
    "- **kwargs: additional args parsed to transforms_imagenet_train and transforms_imagenet_eval.\n",
    "\n",
    "2. The [mindcv.data.create_loader](https://mindcv.readthedocs.io/en/latest/api/mindcv.data.html#mindcv.data.create_loader) function is used for data conversion and batch split loading. We need to pass in the transform_list returned by [create_transforms](https://mindcv.readthedocs.io/en/latest/api/mindcv.data.html#mindcv.data.create_transforms).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data augmentation operations to generate the required dataset.\n",
    "loader_train = create_loader(dataset=dataset_train,\n",
    "                             batch_size=64,\n",
    "                             is_training=True,\n",
    "                             num_classes=num_classes,\n",
    "                             transform=trans,\n",
    "                             num_parallel_workers=num_workers)\n",
    "\n",
    "num_batches = loader_train.get_dataset_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[create_loader](https://mindcv.readthedocs.io/en/latest/api/mindcv.data.html#mindcv.data.create_loader) parameters:\n",
    "\n",
    "- dataset: dataset object created by create_dataset.\n",
    "\n",
    "- batch_size: The number of rows each batch is created with. An int or callable object which takes exactly 1 parameter, BatchInfo.\n",
    "\n",
    "- drop_remainder: Determines whether to drop the last block whose data row number is less than batch size (default=False). If True, and if there are less than batch_size rows available to make the last batch, then those rows will be dropped and not propagated to the child node.\n",
    "\n",
    "- is_training: whether it is in train mode. Default: False.\n",
    "\n",
    "- mixup: mixup alpha, mixup will be enbled if > 0. (default=0.0).\n",
    "\n",
    "- cutmix: cutmix alpha, cutmix will be enabled if > 0. (default=0.0). This operation is experimental.\n",
    "\n",
    "- cutmix_prob: prob of doing cutmix for an image (default=0.0)\n",
    "\n",
    "- num_classes: the number of classes. Default: 1000.\n",
    "    \n",
    "- transform: the list of transformations that wil be applied on the image, which is obtained by create_transform. If None, the default imagenet transformation for evaluation will be applied. Default: None.\n",
    "\n",
    "- target_transform: the list of transformations that will be applied on the label. If None, the label will be converted to the type of ms.int32. Default: None.\n",
    "\n",
    "- num_parallel_workers: Number of workers(threads) to process the dataset in parallel (default=None).\n",
    "\n",
    "- python_multiprocessing: Parallelize Python operations with multiple worker processes. This option could be beneficial if the Python operation is computational heavy (default=False).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Avoid repeatedly executing a single cell of [create_loader](https://mindcv.readthedocs.io/en/latest/api/mindcv.data.html#mindcv.data.create_loader) in notebook, or execute again after executing [create_dataset](https://mindcv.readthedocs.io/en/latest/api/mindcv.data.html#mindcv.data.create_dataset). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation and Loading\n",
    "\n",
    "Use the [create_model](https://mindcv.readthedocs.io/en/latest/api/mindcv.models.html#mindcv.models.create_model) interface to obtain the instantiated DenseNet and load the pretraining weight densenet_121_224.ckpt (obtained from ImageNet dataset training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32293888B [00:01, 28754770.92B/s]                               \n",
      "[WARNING] ME(1769:281472959711936,MainProcess):2022-12-21-16:03:22.690.392 [mindspore/train/serialization.py:712] For 'load_param_into_net', 2 parameters in the 'net' are not loaded, because they are not in the 'parameter_dict', please check whether the network structure is consistent when training and loading checkpoint.\n",
      "[WARNING] ME(1769:281472959711936,MainProcess):2022-12-21-16:03:22.691.960 [mindspore/train/serialization.py:714] classifier.weight is not loaded.\n",
      "[WARNING] ME(1769:281472959711936,MainProcess):2022-12-21-16:03:22.692.908 [mindspore/train/serialization.py:714] classifier.bias is not loaded.\n"
     ]
    }
   ],
   "source": [
    "from mindcv.models import create_model\n",
    "\n",
    "# nstantiate the DenseNet121 model and load the pretraining weights.\n",
    "network = create_model(model_name='densenet121', num_classes=num_classes, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Because the number of classes required by Cifar10 and ImageNet datasets is different, the classifier parameters cannot be shared, and the warning that the classifier parameters cannot be loaded does not affect the fine tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[create_model](https://mindcv.readthedocs.io/en/latest/api/mindcv.models.html#mindcv.models.create_model) parameters:\n",
    "\n",
    "- model_name: The name of model.\n",
    "\n",
    "- num_classes: The number of classes. Default: 1000.\n",
    "\n",
    "- pretrained: Whether to load the pretrained model. Default: False.\n",
    "\n",
    "- in_channels: The input channels. Default: 3.\n",
    "\n",
    "- checkpoint_path: The path of checkpoint files. Default: “”.\n",
    "\n",
    "- use_ema: Whether use ema method. Default: False.\n",
    "\n",
    "Use the [mindcv.loss.create_loss](https://mindcv.readthedocs.io/en/latest/api/mindcv.loss.html#mindcv.loss.create_loss) interface to create a loss function (cross_entropy loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By [create_loss](https://mindcv.readthedocs.io/en/latest/api/mindcv.loss.html#mindcv.loss.create_loss) interface obtains loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindcv.loss import create_loss\n",
    "\n",
    "loss = create_loss(name='CE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[create_loss](https://mindcv.readthedocs.io/en/latest/api/mindcv.loss.html#mindcv.loss.create_loss) parameters:\n",
    "\n",
    "- name: loss name, ‘CE’ for cross_entropy. ‘BCE’: binary cross entropy. Default: ‘CE’.\n",
    "\n",
    "- weight: Class weight. A rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size ‘nbatch’. Data type must be float16 or float32.\n",
    "\n",
    "- reduction: Apply specific reduction method to the output: ‘mean’ or ‘sum’. By default, the sum of the output will be divided by the number of elements in the output. ‘sum’: the output will be summed. Default:’mean’.\n",
    "\n",
    "- label_smoothing: Label smoothing factor, a regularization tool used to prevent the model from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default: 0.0.\n",
    "\n",
    "- aux_factor: Auxiliary loss factor. Set aux_fuactor > 0.0 if the model has auxilary logit outputs (i.e., deep supervision), like inception_v3. Default: 0.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [create_scheduler](https://mindcv.readthedocs.io/en/latest/api/mindcv.scheduler.html#mindcv.scheduler.create_scheduler) interface sets the learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mindcv.scheduler import create_scheduler\n",
    "\n",
    "# learning rate scheduler\n",
    "lr_scheduler = create_scheduler(steps_per_epoch=num_batches,\n",
    "                                scheduler='constant',\n",
    "                                lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[create_scheduler](https://mindcv.readthedocs.io/en/latest/api/mindcv.scheduler.html#mindcv.scheduler.create_scheduler) parameters:\n",
    "\n",
    "- steps_pre_epoch: number of steps per epoch.\n",
    "\n",
    "- scheduler: scheduler name like ‘constant’, ‘cosine_decay’, ‘step_decay’, ‘exponential_decay’, ‘polynomial_decay’, ‘multi_step_decay’. Default: ‘constant’.\n",
    "\n",
    "- lr: learning rate value. Default: 0.01.\n",
    "\n",
    "- min_lr: lower lr bound for ‘cosine_decay’ schedulers. Default: 1e-6.\n",
    "\n",
    "- warmup_epochs: epochs to warmup LR, if scheduler supports. Default: 3.\n",
    "\n",
    "- warmup_factor: the warmup phase of scheduler is a linearly increasing lr, the beginning factor is warmup_factor, i.e., the lr of the first step/epoch is lr*warmup_factor, and the ending lr in the warmup phase is lr. Default: 0.0.\n",
    "\n",
    "- decay_epochs: for ‘cosine_decay’ schedulers, decay LR to min_lr in decay_epochs. For ‘step_decay’ scheduler, decay LR by a factor of decay_rate every decay_epochs. Default: 10.\n",
    "\n",
    "- decay_rate: LR decay rate (default: 0.9).\n",
    "\n",
    "- milestones: list of epoch milestones for ‘multi_step_decay’ scheduler. Must be increasing.\n",
    "\n",
    "- num_epochs: number of total epochs.\n",
    "\n",
    "- lr_epoch_stair: If True, LR will be updated in the beginning of each new epoch and the LR will be consistent for each batch in one epoch. Otherwise, learning rate will be updated dynamically in each step. (default=False).\n",
    "\n",
    "\n",
    "Use [create_optimizer](https://mindcv.readthedocs.io/en/latest/api/mindcv.optim.html#mindcv.optim.create_optimizer) interface creates an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindcv.optim import create_optimizer\n",
    "\n",
    "# create optimizer\n",
    "opt = create_optimizer(network.trainable_params(), opt='adam', lr=lr_scheduler) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[create_optimizer](https://mindcv.readthedocs.io/en/latest/api/mindcv.optim.html#mindcv.optim.create_optimizer) parameters:\n",
    "\n",
    "- params: network parameters. Union[list[Parameter],list[dict]], which must be the list of parameters or list of dicts. When the list element is a dictionary, the key of the dictionary can be “params”, “lr”, “weight_decay”,”grad_centralization” and “order_params”.\n",
    "\n",
    "- opt: Wrapped optimizer. You could choose like ‘sgd’, ‘nesterov’, ‘momentum’, ‘adam’, ‘adamw’, ‘rmsprop’, ‘adagrad’, ‘lamb’. ‘adam’ is the default choise for convolution-based networks. ‘adamw’ is recommended for ViT-based networks. Default: ‘adam’.\n",
    "\n",
    "- lr: learning rate, float or lr scheduler. Fixed and dynamic learning rate are supported. Default: 1e-3.\n",
    "\n",
    "- weight_decay: weight decay factor. It should be noted that weight decay can be a constant value or a Cell. It is a Cell only when dynamic weight decay is applied. Dynamic weight decay is similar to dynamic learning rate, users need to customize a weight decay schedule only with global step as input, and during training, the optimizer calls the instance of WeightDecaySchedule to get the weight decay value of current step. Default: 0.\n",
    "\n",
    "- momentum: momentum if the optimizer supports. Default: 0.9.\n",
    "\n",
    "- nesterov: whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients. Default: False.\n",
    "\n",
    "- filter_bias_and_bn: whether to filter batch norm paramters and bias from weight decay. If True, weight decay will not apply on BN parameters and bias in Conv or Dense layers. Default: True.\n",
    "\n",
    "- loss_scale: A floating point value for the loss scale, which must be larger than 0.0. Default: 1.0.\n",
    "\n",
    "- checkpoint_path: optimizer checkpoint path.\n",
    "\n",
    "- eps: Term Added to the Denominator to Improve Numerical Stability. default: 1e-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [mindspore.Model](https://mindspore.cn/docs/zh-CN/r1.8/api_python/mindspore/mindspore.Model.html) interface to encapsulate trainable instances according to the parameters passed in by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import Model\n",
    "\n",
    "# Encapsulates examples that can be trained or inferred\n",
    "model = Model(network, loss_fn=loss, optimizer=opt, metrics={'accuracy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [`mindspore.Model.train`](https://mindspore.cn/docs/zh-CN/r1.8/api_python/mindspore/mindspore.Model.html#mindspore.Model.train) interface for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:04:30.001.890 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op5273] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 156, loss is 2.0816354751586914\n",
      "epoch: 1 step: 312, loss is 1.4474115371704102\n",
      "epoch: 1 step: 468, loss is 0.8935483694076538\n",
      "epoch: 1 step: 624, loss is 0.5588696002960205\n",
      "epoch: 1 step: 780, loss is 0.3161369860172272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:09:20.261.851 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op16720] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch time: 416429.509 ms, per step time: 532.519 ms\n",
      "epoch: 2 step: 154, loss is 0.19752007722854614\n",
      "epoch: 2 step: 310, loss is 0.14635677635669708\n",
      "epoch: 2 step: 466, loss is 0.3511860966682434\n",
      "epoch: 2 step: 622, loss is 0.12542471289634705\n",
      "epoch: 2 step: 778, loss is 0.22351759672164917\n",
      "Train epoch time: 156746.872 ms, per step time: 200.444 ms\n",
      "epoch: 3 step: 152, loss is 0.08965137600898743\n",
      "epoch: 3 step: 308, loss is 0.22765043377876282\n",
      "epoch: 3 step: 464, loss is 0.19035443663597107\n",
      "epoch: 3 step: 620, loss is 0.06591956317424774\n",
      "epoch: 3 step: 776, loss is 0.0934530645608902\n",
      "Train epoch time: 156574.210 ms, per step time: 200.223 ms\n",
      "epoch: 4 step: 150, loss is 0.03782692924141884\n",
      "epoch: 4 step: 306, loss is 0.023876197636127472\n",
      "epoch: 4 step: 462, loss is 0.038690414279699326\n",
      "epoch: 4 step: 618, loss is 0.15388774871826172\n",
      "epoch: 4 step: 774, loss is 0.1581358164548874\n",
      "Train epoch time: 158398.108 ms, per step time: 202.555 ms\n",
      "epoch: 5 step: 148, loss is 0.06556802988052368\n",
      "epoch: 5 step: 304, loss is 0.006707251071929932\n",
      "epoch: 5 step: 460, loss is 0.02353120595216751\n",
      "epoch: 5 step: 616, loss is 0.014183484017848969\n",
      "epoch: 5 step: 772, loss is 0.09367241710424423\n",
      "Train epoch time: 154978.618 ms, per step time: 198.182 ms\n"
     ]
    }
   ],
   "source": [
    "from mindspore import LossMonitor, TimeMonitor, CheckpointConfig, ModelCheckpoint\n",
    "\n",
    "# Set the callback function for saving network parameters during training.\n",
    "ckpt_save_dir = './ckpt' \n",
    "ckpt_config = CheckpointConfig(save_checkpoint_steps=num_batches)\n",
    "ckpt_cb = ModelCheckpoint(prefix='densenet121-cifar10',\n",
    "                          directory=ckpt_save_dir,\n",
    "                          config=ckpt_config)\n",
    "\n",
    "model.train(5, loader_train, callbacks=[LossMonitor(num_batches//5), TimeMonitor(num_batches//5), ckpt_cb], dataset_sink_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation dataset\n",
    "dataset_val = create_dataset(name='cifar10', root=cifar10_dir, split='test', shuffle=True, num_parallel_workers=num_workers, download=download)\n",
    "\n",
    "# Perform data enhancement operations to generate the required dataset.\n",
    "loader_val = create_loader(dataset=dataset_val,\n",
    "                           batch_size=64,\n",
    "                           is_training=False,\n",
    "                           num_classes=num_classes,\n",
    "                           transform=trans,\n",
    "                           num_parallel_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the fine-tuning parameter file (densenet121-cifar10-5_782.ckpt) to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encapsulate inferable instances according to the parameters passed in by the user, load the validation data set, and verify the precision of the fine tuned DenseNet121 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:24:11.927.472 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op24314] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.951}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:25:01.871.273 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op27139] don't support int64, reduce precision from int64 to int32.\n"
     ]
    }
   ],
   "source": [
    "# Verify the accuracy of DenseNet121 after fine-tune\n",
    "acc = model.eval(loader_val, dataset_sink_mode=False)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use YAML files for model training and validation\n",
    "\n",
    "We can also use the yaml file with the model parameters set directly to quickly train and verify the model through `train.py` and `validate.py` scripts. The following is an example of training SqueezenetV1 on [ImageNet](https://www.image-net.org/challenges/LSVRC/2012/index.php) (you need to download ImageNet to the directory in advance)\n",
    "\n",
    "\n",
    "> For detailed tutorials, please refer to the [tutorial](https://mindcv.readthedocs.io/en/latest/tutorials/learn_about_config.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/mindspore-lab/mindcv.git\n",
    "!cd mindcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standalone training on a CPU/GPU/Ascend device\n",
    "!python train.py -c configs/squeezenet/squeezenet_1.0_gpu.yaml --data_dir /path/to/dataset --distribute False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python validate.py -c configs/squeezenet/squeezenet_1.0_gpu.yaml --data_dir /path/to/dataset --ckpt_path /path/to/ckpt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
