{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Model Configuration\n",
    "\n",
    "`mindcv` can parse the yaml file of the model through the `argparse` library and `pyyaml` library to configure parameters. Let's use squeezenet_1.0 model as an example to explain how to configure the corresponding parameters.\n",
    "\n",
    "\n",
    "## Basic Environment\n",
    "\n",
    "1. Parameter description\n",
    "\n",
    "- mode: Use graph mode (0) or pynative mode (1).\n",
    "\n",
    "- distribute: Whether to use distributed.\n",
    "\n",
    "\n",
    "2. Sample yaml file\n",
    "\n",
    "```text\n",
    "mode: 0 \n",
    "distribute: True \n",
    "...\n",
    "```\n",
    "\n",
    "3. Parse parameter setting\n",
    "\n",
    "```text \n",
    "python train.py --mode 0 --distribute False ...\n",
    "```\n",
    "\n",
    "4. Corresponding code example\n",
    "\n",
    "> `args.model` represents the parameter `mode`, `args.distribute` represents the parameter `distribute`。\n",
    "\n",
    "```python\n",
    "def train(args):\n",
    "    ms.set_context(mode=args.mode)\n",
    "\n",
    "    if args.distribute:\n",
    "        init()\n",
    "        device_num = get_group_size()\n",
    "        rank_id = get_rank()\n",
    "        ms.set_auto_parallel_context(device_num=device_num,\n",
    "                                     parallel_mode='data_parallel',\n",
    "                                     gradients_mean=True)\n",
    "    else:\n",
    "        device_num = None\n",
    "        rank_id = None\n",
    "    ...\n",
    "```\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "1. Parameter description\n",
    "\n",
    "- dataset: dataset name.\n",
    "\n",
    "- data_dir: Path of dataset file.\n",
    "\n",
    "- shuffle: whether to shuffle the dataset.\n",
    "\n",
    "- dataset_download: whether to download the dataset.\n",
    "\n",
    "- batch_size: The number of rows each batch.\n",
    "\n",
    "- drop_remainder: Determines whether to drop the last block whose data row number is less than batch size.\n",
    "\n",
    "- num_parallel_workers: Number of workers(threads) to process the dataset in parallel.\n",
    "\n",
    "\n",
    "2. Sample yaml file\n",
    "\n",
    "```text\n",
    "dataset: 'imagenet'\n",
    "data_dir: './imagenet2012'\n",
    "shuffle: True\n",
    "dataset_download: False\n",
    "batch_size: 32\n",
    "drop_remainder: True \n",
    "num_parallel_workers: 8\n",
    "...\n",
    "```\n",
    "\n",
    "3. Parse parameter setting\n",
    "\n",
    "```text \n",
    "python train.py ... --dataset imagenet --data_dir ./imagenet2012 --shuffle True \\\n",
    "            --dataset_download False --batch_size 32 --drop_remainder True \\\n",
    "            --num_parallel_workers 8 ...\n",
    "```\n",
    "\n",
    "4. Corresponding code example\n",
    "\n",
    "```python\n",
    "def train(args):\n",
    "    ...\n",
    "    dataset_train = create_dataset(\n",
    "        name=args.dataset,\n",
    "        root=args.data_dir,\n",
    "        split='train',\n",
    "        shuffle=args.shuffle,\n",
    "        num_samples=args.num_samples,\n",
    "        num_shards=device_num,\n",
    "        shard_id=rank_id,\n",
    "        num_parallel_workers=args.num_parallel_workers,\n",
    "        download=args.dataset_download,\n",
    "        num_aug_repeats=args.aug_repeats)\n",
    "    \n",
    "    ...\n",
    "    target_transform = transforms.OneHot(num_classes) if args.loss == 'BCE' else None\n",
    "    \n",
    "    loader_train = create_loader(\n",
    "        dataset=dataset_train,\n",
    "        batch_size=args.batch_size,\n",
    "        drop_remainder=args.drop_remainder,\n",
    "        is_training=True,\n",
    "        mixup=args.mixup,\n",
    "        cutmix=args.cutmix,\n",
    "        cutmix_prob=args.cutmix_prob,\n",
    "        num_classes=args.num_classes,\n",
    "        transform=transform_list,\n",
    "        target_transform=target_transform,\n",
    "        num_parallel_workers=args.num_parallel_workers,\n",
    "    )\n",
    "    \n",
    "    ...\n",
    "```\n",
    "\n",
    "## Data Augmentation\n",
    "\n",
    "1. Parameter description\n",
    "\n",
    "- image_resize: the image size after resize for adapting to network.\n",
    "\n",
    "- scale: random resize scale.\n",
    "\n",
    "- ratio: random resize aspect ratio.\n",
    "\n",
    "- hfilp: horizontal flip training aug probability.\n",
    "\n",
    "- interpolation: image interpolation mode for resize operator.\n",
    "\n",
    "- crop_pct: input image center crop percent.\n",
    "\n",
    "- color_jitter: color jitter factor.\n",
    "\n",
    "- re_prob: probability of performing erasing.\n",
    "\n",
    "2. Sample yaml file\n",
    "\n",
    "```text\n",
    "image_resize: 224\n",
    "scale: [0.08, 1.0]\n",
    "ratio: [0.75, 1.333]\n",
    "hflip: 0.5\n",
    "interpolation: 'bilinear'\n",
    "crop_pct: 0.875\n",
    "color_jitter: [0.4, 0.4, 0.4]\n",
    "re_prob: 0.5\n",
    "...\n",
    "```\n",
    "\n",
    "3. Parse parameter setting\n",
    "\n",
    "```text\n",
    "python train.py ... --image_resize 224 --scale [0.08, 1.0] --ratio [0.75, 1.333] \\\n",
    "            --hflip 0.5 --interpolation \"bilinear\" --crop_pct 0.875 \\\n",
    "            --color_jitter [0.4, 0.4, 0.4] --re_prob 0.5 ...\n",
    "```\n",
    "\n",
    "4. Corresponding code example\n",
    "\n",
    "```python\n",
    "def train(args):\n",
    "    ...\n",
    "    transform_list = create_transforms(\n",
    "        dataset_name=args.dataset,\n",
    "        is_training=True,\n",
    "        image_resize=args.image_resize,\n",
    "        scale=args.scale,\n",
    "        ratio=args.ratio,\n",
    "        hflip=args.hflip,\n",
    "        vflip=args.vflip,\n",
    "        color_jitter=args.color_jitter,\n",
    "        interpolation=args.interpolation,\n",
    "        auto_augment=args.auto_augment,\n",
    "        mean=args.mean,\n",
    "        std=args.std,\n",
    "        re_prob=args.re_prob,\n",
    "        re_scale=args.re_scale,\n",
    "        re_ratio=args.re_ratio,\n",
    "        re_value=args.re_value,\n",
    "        re_max_attempts=args.re_max_attempts\n",
    "    )\n",
    "    ...\n",
    "```\n",
    "\n",
    "## Model\n",
    "\n",
    "1. Parameter description\n",
    "\n",
    "- model: model name。\n",
    "\n",
    "- num_classes: number of label classes.。\n",
    "\n",
    "- pretrained: whether load pretrained model。\n",
    "\n",
    "- ckpt_path: initialize model from this checkpoint.。\n",
    "\n",
    "- keep_checkpoint_max: max number of checkpoint files。\n",
    "\n",
    "- ckpt_save_dir: path of checkpoint.\n",
    "\n",
    "- epoch_size: train epoch size.\n",
    "\n",
    "- dataset_sink_mode: the dataset sink mode。\n",
    "\n",
    "- amp_level: auto mixed precision level for saving memory and acceleration.\n",
    "\n",
    "2. Sample yaml file\n",
    "\n",
    "```text\n",
    "model: 'squeezenet1_0'\n",
    "num_classes: 1000\n",
    "pretrained: False\n",
    "ckpt_path: './squeezenet1_0_gpu.ckpt'\n",
    "keep_checkpoint_max: 10\n",
    "ckpt_save_dir: './ckpt/'\n",
    "epoch_size: 200\n",
    "dataset_sink_mode: True\n",
    "amp_level: 'O0'\n",
    "...\n",
    "```\n",
    "\n",
    "3. Parse parameter setting\n",
    "\n",
    "```text\n",
    "python train.py ... --model squeezenet1_0 --num_classes 1000 --pretrained False \\\n",
    "            --ckpt_path ./squeezenet1_0_gpu.ckpt --keep_checkpoint_max 10 \\\n",
    "            --ckpt_save_path ./ckpt/ --epoch_size 200 --dataset_sink_mode True \\\n",
    "            --amp_level O0 ...\n",
    "```\n",
    "\n",
    "4. Corresponding code example\n",
    "\n",
    "```python\n",
    "def train(args):\n",
    "    ...\n",
    "    network = create_model(model_name=args.model,\n",
    "                    num_classes=args.num_classes,\n",
    "                    in_channels=args.in_channels,\n",
    "                    drop_rate=args.drop_rate,\n",
    "                    drop_path_rate=args.drop_path_rate,\n",
    "                    pretrained=args.pretrained,\n",
    "                    checkpoint_path=args.ckpt_path,\n",
    "                    use_ema=args.use_ema)\n",
    "    ...\n",
    "```\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "1. Parameter description\n",
    "\n",
    "- loss: name of loss function, BCE (BinaryCrossEntropy) or CE (CrossEntropy).\n",
    "\n",
    "- label_smoothing: use label smoothing.\n",
    "\n",
    "2. Sample yaml file\n",
    "\n",
    "```text\n",
    "loss: 'CE'\n",
    "label_smoothing: 0.1\n",
    "...\n",
    "```\n",
    "\n",
    "3. Parse parameter setting\n",
    "\n",
    "```text\n",
    "python train.py ... --loss CE --label_smoothing 0.1 ...\n",
    "```\n",
    "\n",
    "4. Corresponding code example\n",
    "\n",
    "```python\n",
    "def train(args):\n",
    "    ...\n",
    "    loss = create_loss(name=args.loss,\n",
    "                 reduction=args.reduction,\n",
    "                 label_smoothing=args.label_smoothing,\n",
    "                 aux_factor=args.aux_factor)\n",
    "    ...\n",
    "```\n",
    "\n",
    "## Learning Rate Scheduler\n",
    "\n",
    "1. Parameter description\n",
    "\n",
    "- scheduler: name of scheduler.\n",
    "\n",
    "- min_lr: the minimum value of learning rate if scheduler supports.\n",
    "\n",
    "- lr: learning rate.\n",
    "\n",
    "- warmup_epochs: warmup epochs.\n",
    "\n",
    "- decay_epochs: decay epochs.\n",
    "\n",
    "2. Sample yaml file\n",
    "\n",
    "```text\n",
    "scheduler: 'cosine_decay'\n",
    "min_lr: 0.0\n",
    "lr: 0.01\n",
    "warmup_epochs: 0\n",
    "decay_epochs: 200\n",
    "...\n",
    "```\n",
    "\n",
    "3. Parse parameter setting\n",
    "\n",
    "```text\n",
    "python train.py ... --scheduler cosine_decay --min_lr 0.0 --lr 0.01 \\\n",
    "             --warmup_epochs 0 --decay_epochs 200 ...\n",
    "```\n",
    "\n",
    "4. Corresponding code example\n",
    "\n",
    "```python\n",
    "def train(args):\n",
    "    ...\n",
    "    lr_scheduler = create_scheduler(num_batches,\n",
    "                          scheduler=args.scheduler,\n",
    "                          lr=args.lr,\n",
    "                          min_lr=args.min_lr,\n",
    "                          warmup_epochs=args.warmup_epochs,\n",
    "                          warmup_factor=args.warmup_factor,\n",
    "                          decay_epochs=args.decay_epochs,\n",
    "                          decay_rate=args.decay_rate,\n",
    "                          milestones=args.multi_step_decay_milestones,\n",
    "                          num_epochs=args.epoch_size,\n",
    "                          lr_epoch_stair=args.lr_epoch_stair)\n",
    "    ...\n",
    "```\n",
    "\n",
    "\n",
    "## optimizer\n",
    "\n",
    "1. Parameter description\n",
    "\n",
    "- opt: name of optimizer。\n",
    "\n",
    "- filter_bias_and_bn: filter Bias and BatchNorm.\n",
    "\n",
    "- momentum: Hyperparameter of type float, means momentum for the moving average.\n",
    "\n",
    "- weight_decay: weight decay（L2 penalty）。\n",
    "\n",
    "- loss_scale: gradient scaling factor\n",
    "\n",
    "- use_nesterov: whether enables the Nesterov momentum\n",
    "\n",
    "2. Sample yaml file\n",
    "\n",
    "```text\n",
    "opt: 'momentum'\n",
    "filter_bias_and_bn: True\n",
    "momentum: 0.9\n",
    "weight_decay: 0.00007\n",
    "loss_scale: 1024\n",
    "use_nesterov: False\n",
    "...\n",
    "```\n",
    "\n",
    "3. Parse parameter setting\n",
    "\n",
    "```text\n",
    "python train.py ... --opt momentum --filter_bias_and_bn True --weight_decay 0.00007 \\\n",
    "              --loss_scale 1024 --use_nesterov False ...\n",
    "```\n",
    "\n",
    "4. Corresponding code example\n",
    "\n",
    "```python\n",
    "def train(args):\n",
    "    ...\n",
    "    if args.use_ema:\n",
    "        optimizer = create_optimizer(network.trainable_params(),\n",
    "                            opt=args.opt,\n",
    "                            lr=lr_scheduler,\n",
    "                            weight_decay=args.weight_decay,\n",
    "                            momentum=args.momentum,\n",
    "                            nesterov=args.use_nesterov,\n",
    "                            filter_bias_and_bn=args.filter_bias_and_bn,\n",
    "                            loss_scale=args.loss_scale,\n",
    "                            checkpoint_path=opt_ckpt_path,\n",
    "                            eps=args.eps)\n",
    "     else:\n",
    "        optimizer = create_optimizer(network.trainable_params(),\n",
    "                            opt=args.opt,\n",
    "                            lr=lr_scheduler,\n",
    "                            weight_decay=args.weight_decay,\n",
    "                            momentum=args.momentum,\n",
    "                            nesterov=args.use_nesterov,\n",
    "                            filter_bias_and_bn=args.filter_bias_and_bn,\n",
    "                            checkpoint_path=opt_ckpt_path,\n",
    "                            eps=args.eps)\n",
    "    ...\n",
    "```\n",
    "\n",
    "\n",
    "## Combination of Yaml and Parse\n",
    "\n",
    "You can override the parameter settings in the yaml file by using parse to set parameters. Take the following shell command as an example,\n",
    "\n",
    "```shell\n",
    "python train.py -c ./configs/squeezenet/squeezenet_1.0_gpu.yaml --data_dir ./data\n",
    "```\n",
    "The above command overwrites the value of `args.data_dir` parameter from ./imaget2012 in yaml file to ./data.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
